{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Inference on a Pendulum using DiffEqBayes.jl\n### Vaibhav Dixit\n\n### Set up simple pendulum problem"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DiffEqBayes, OrdinaryDiffEq, RecursiveArrayTools, Distributions, Plots, StatsPlots, BenchmarkTools, TransformVariables, CmdStan, DynamicHMC"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define our simple pendulum problem. Here our pendulum has a drag term `ω`\nand a length `L`.\n\n![pendulum](https://user-images.githubusercontent.com/1814174/59942945-059c1680-942f-11e9-991c-2025e6e4ccd3.jpg)\n\nWe get first order equations by defining the first term as the velocity and the\nsecond term as the position, getting:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function pendulum(du,u,p,t)\n    ω,L = p\n    x,y = u\n    du[1] = y\n    du[2] = - ω*y -(9.8/L)*sin(x)\nend\n\nu0 = [1.0,0.1]\ntspan = (0.0,10.0)\nprob1 = ODEProblem(pendulum,u0,tspan,[1.0,2.5])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solve the model and plot\n\nTo understand the model and generate data, let's solve and visualize the solution\nwith the known parameters:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sol = solve(prob1,Tsit5())\nplot(sol)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's the pendulum, so you know what it looks like. It's periodic, but since we\nhave not made a small angle assumption it's not exactly `sin` or `cos`. Because\nthe true dampening parameter `ω` is 1, the solution does not decay over time,\nnor does it increase. The length `L` determines the period.\n\n### Create some dummy data to use for estimation\n\nWe now generate some dummy data to use for estimation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "t = collect(range(1,stop=10,length=10))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what our data looks like on top of the real solution"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "scatter!(data')"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data captures the non-dampening effect and the true period, making it\nperfect to attempting a Bayesian inference.\n\n### Perform Bayesian Estimation\n\nNow let's fit the pendulum to the data. Since we know our model is correct,\nthis should give us back the parameters that we used to generate the data!\nDefine priors on our parameters. In this case, let's assume we don't have much\ninformation, but have a prior belief that ω is between 0.1 and 3.0, while the\nlength of the pendulum L is probably around 3.0:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "priors = [Uniform(0.1,3.0), Normal(3.0,1.0)]"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally let's run the estimation routine from DiffEqBayes.jl with the Turing.jl backend to check if we indeed recover the parameters!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bayesian_result = turing_inference(prob1,Tsit5(),t,data,priors;num_samples=10_000,\n                                   syms = [:omega,:L])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that while our guesses had the wrong means, the learned parameters converged\nto the correct means, meaning that it learned good posterior distributions for the\nparameters. To look at these posterior distributions on the parameters, we can\nexamine the chains:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(bayesian_result)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a diagnostic, we will also check the parameter chains. The chain is the MCMC\nsampling process. The chain should explore parameter space and converge reasonably\nwell, and we should be taking a lot of samples after it converges (it is these\nsamples that form the posterior distribution!)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(bayesian_result, colordim = :parameter)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that after awhile these chains converge to a \"fuzzy line\", meaning it\nfound the area with the most likelihood and then starts to sample around there,\nwhich builds a posterior distribution around the true mean.\n\nDiffEqBayes.jl allows the choice of using Stan.jl, Turing.jl and DynamicHMC.jl for MCMC, you can also use ApproxBayes.jl for Approximate Bayesian computation algorithms.\nLet's compare the timings across the different MCMC backends. We'll stick with the default arguments and 10,000 samples in each since there is a lot of room for micro-optimization\nspecific to each package and algorithm combinations, you might want to do your own experiments for specific problems to get better understanding of the performance."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime bayesian_result = turing_inference(prob1,Tsit5(),t,data,priors;syms = [:omega,:L],num_samples=10_000)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime bayesian_result = stan_inference(prob1,t,data,priors;num_samples=10_000,printsummary=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime bayesian_result = dynamichmc_inference(prob1,Tsit5(),t,data,priors;num_samples = 10_000)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.4.0"
    },
    "kernelspec": {
      "name": "julia-1.4",
      "display_name": "Julia 1.4.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
